---
title: "Python Quickstart"
description: "Get up and running using our Python SDK"
---

# Installation
`pip install vocode`

## Conversation Class

The Python SDK contains a `Conversation` class that allows you to 
spin up conversations quickly. The abstraction requires the following fields:

<ParamField path="input_device" type="InputDevice" required={true}>
  The microphone input device for the conversation.
</ParamField>

<ParamField path="output_device" type="OutputDevice" required={true}>
  The speaker output device for the conversation.
</ParamField>

<ParamField path="transcriber_config" type="TranscriberConfig" required={true}>
  The configuration for speech recognition.
</ParamField>

<ParamField path="agent_config" type="AgentConfig" required={true}>
  The configuration for the AI agent.
</ParamField>

<ParamField path="synthesizer_config" type="SynthesizerConfig" required={true}>
  The configuration for speech synthesis.
</ParamField>

## Example: run a conversation in your terminal
First, we first need to create the IO devices used for
the conversation. Vocode provides a simple helper method to do so for the 
default IO devices on your computer:
```python 
create_microphone_input_and_speaker_output(use_first_available_device=True)
```
Then, we need to define the `Conversation` class:
```python 
conversation = Conversation(
    input_device=microphone_input,
    output_device=speaker_output,
    transcriber_config=DeepgramTranscriberConfig.from_input_device(microphone_input),
    agent_config=LLMAgentConfig(prompt_preamble="The AI is having a pleasant conversation about life."),
    synthesizer_config=AzureSynthesizerConfig.from_output_device(speaker_output)
)
```
Here, we use Deepgram for transcription, an LLM Agent, and Azure for speech synthesis. 
Putting it all together in a file you can run:

```python

import asyncio
import signal

from vocode.conversation import Conversation
from vocode.helpers import create_microphone_input_and_speaker_output
from vocode.models.transcriber import DeepgramTranscriberConfig
from vocode.models.agent import LLMAgentConfig
from vocode.models.synthesizer import AzureSynthesizerConfig

if __name__ == "__main__":
    microphone_input, speaker_output = create_microphone_input_and_speaker_output(use_first_available_device=True)

    conversation = Conversation(
        input_device=microphone_input,
        output_device=speaker_output,
        transcriber_config=DeepgramTranscriberConfig.from_input_device(microphone_input),
        agent_config=LLMAgentConfig(prompt_preamble="The AI is having a pleasant conversation about life."),
        synthesizer_config=AzureSynthesizerConfig.from_output_device(speaker_output)
    )
    signal.signal(signal.SIGINT, lambda _0, _1: conversation.deactivate())
    asyncio.run(conversation.start())
```